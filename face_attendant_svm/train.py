import os
import numpy as np
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
import joblib
import time

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
MODELS_DIR = os.path.join(BASE_DIR, "models")

LARGE_DATASET_THRESHOLD = 1000
KNN_NEIGHBORS = 5

def load_data():
    emb_path = os.path.join(DATA_DIR, "embeddings.npy")
    lbl_path = os.path.join(DATA_DIR, "labels.npy")
    if not (os.path.exists(emb_path) and os.path.exists(lbl_path)):
        raise FileNotFoundError("Ch∆∞a c√≥ d·ªØ li·ªáu. H√£y ch·∫°y registerFace.py ƒë·ªÉ thu th·∫≠p tr∆∞·ªõc.")
    X = np.load(emb_path)
    y = np.load(lbl_path, allow_pickle=True)
    if X.ndim != 2:
        raise ValueError("Embeddings ph·∫£i l√† m·∫£ng 2 chi·ªÅu [N, D].")
    if X.shape[0] != y.shape[0]:
        raise ValueError("S·ªë l∆∞·ª£ng embeddings v√† labels kh√¥ng kh·ªõp.")
    return X, y

def normalize_data(X):
    scaler = StandardScaler()
    Xn = scaler.fit_transform(X)
    return Xn, scaler

def train_svm(X_train, y_train, use_rbf=False):
    if use_rbf:
        svm_model = SVC(kernel='rbf', gamma='scale', probability=True, cache_size=500)
    else:
        svm_model = SVC(kernel='linear', probability=True, cache_size=500)
    svm_model.fit(X_train, y_train)
    return svm_model

def train_knn(X_train, y_train, n_neighbors=5):
    knn_model = KNeighborsClassifier(
        n_neighbors=n_neighbors,
        algorithm='ball_tree',
        metric='euclidean',
        n_jobs=-1
    )
    knn_model.fit(X_train, y_train)
    return knn_model

def select_best_model(X, y, X_train, X_test, y_train, y_test):
    num_samples = X.shape[0]
    
    if num_samples >= LARGE_DATASET_THRESHOLD:
        print(f"D·ªØ li·ªáu l·ªõn ({num_samples} m·∫´u) - S·ª≠ d·ª•ng KNN")
        start = time.time()
        model = train_knn(X_train, y_train, n_neighbors=min(KNN_NEIGHBORS, len(np.unique(y_train))))
        train_time = time.time() - start
        model_type = 'knn'
    elif num_samples >= 300:
        print(f"D·ªØ li·ªáu trung b√¨nh ({num_samples} m·∫´u) - S·ª≠ d·ª•ng SVM-RBF")
        start = time.time()
        model = train_svm(X_train, y_train, use_rbf=True)
        train_time = time.time() - start
        model_type = 'svm_rbf'
    else:
        print(f"D·ªØ li·ªáu nh·ªè ({num_samples} m·∫´u) - S·ª≠ d·ª•ng SVM-Linear")
        start = time.time()
        model = train_svm(X_train, y_train, use_rbf=False)
        train_time = time.time() - start
        model_type = 'svm_linear'
    
    print(f"Th·ªùi gian training: {train_time:.2f}s")
    return model, model_type

def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {acc:.4f}")
    try:
        print(classification_report(y_test, y_pred))
    except Exception:
        pass

def main():
    os.makedirs(MODELS_DIR, exist_ok=True)
    print("ƒêang t·∫£i d·ªØ li·ªáu...")
    X, y = load_data()
    print(f"T·ªïng m·∫´u: {X.shape[0]}, K√≠ch th∆∞·ªõc embedding: {X.shape[1]}")
    
    # Ki·ªÉm tra s·ªë l∆∞·ª£ng class
    unique_labels = np.unique(y)
    num_classes = len(unique_labels)
    print(f"S·ªë l∆∞·ª£ng sinh vi√™n: {num_classes}")
    print(f"Danh s√°ch ID: {list(unique_labels)}")
    
    if num_classes < 2:
        print("\n  C·∫¢NH B√ÅO: Ch·ªâ c√≥ 1 sinh vi√™n ƒë∆∞·ª£c ƒëƒÉng k√Ω!")
        print(" SVM c·∫ßn √≠t nh·∫•t 2 class ƒë·ªÉ hu·∫•n luy·ªán.")
        print("\n G·ª¢I √ù GI·∫¢I PH√ÅP:")
        print("1. ƒêƒÉng k√Ω th√™m √≠t nh·∫•t 1 sinh vi√™n n·ªØa")
        print("2. Ho·∫∑c s·ª≠ d·ª•ng m√¥ h√¨nh Simple Matching thay th·∫ø")
        
        choice = input("\n B·∫°n c√≥ mu·ªën s·ª≠ d·ª•ng Simple Matching kh√¥ng? (y/N): ").strip().lower()
        
        if choice in ['y', 'yes']:
            # L∆∞u m√¥ h√¨nh simple matching
            simple_model = {
                'type': 'simple_matching',
                'reference_embedding': X[0],  # L·∫•y embedding ƒë·∫ßu ti√™n l√†m reference
                'student_id': y[0],
                'threshold': 0.6  # Ng∆∞·ª°ng similarity
            }
            
            model_path = os.path.join(MODELS_DIR, "simple_model.pkl")
            joblib.dump(simple_model, model_path)
            
            print(" ƒê√£ l∆∞u Simple Matching model")
            print("üí° Model n√†y s·∫Ω so s√°nh v·ªõi embedding reference")
            print(f"üìÅ L∆∞u t·∫°i: {model_path}")
            return
        else:
            print(" Kh√¥ng th·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh. Vui l√≤ng ƒëƒÉng k√Ω th√™m sinh vi√™n!")
            return

    print("Chu·∫©n h√≥a d·ªØ li·ªáu...")
    Xn, scaler = normalize_data(X)

    X_train, X_test, y_train, y_test = train_test_split(Xn, y, test_size=0.2, random_state=42, stratify=y)

    print("Hu·∫•n luy·ªán model t·ªëi ∆∞u...")
    model, model_type = select_best_model(X, y, X_train, X_test, y_train, y_test)

    print("ƒê√°nh gi√°...")
    evaluate_model(model, X_test, y_test)

    model_path = os.path.join(MODELS_DIR, "svm_model.pkl")
    scaler_path = os.path.join(MODELS_DIR, "normalizer.pkl")
    metadata_path = os.path.join(MODELS_DIR, "model_metadata.pkl")
    
    metadata = {
        'model_type': model_type,
        'num_samples': X.shape[0],
        'num_classes': num_classes,
        'feature_dim': X.shape[1],
        'trained_at': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    joblib.dump(model, model_path)
    joblib.dump(scaler, scaler_path)
    joblib.dump(metadata, metadata_path)
    
    print(f"ƒê√£ l∆∞u model ({model_type}): {model_path}")
    print(f"ƒê√£ l∆∞u scaler: {scaler_path}")
    print(f"ƒê√£ l∆∞u metadata: {metadata_path}")
    print("Ho√†n t·∫•t.")

if __name__ == "__main__":
    main()